# Environment Reset System Guide

This guide explains the environment reset and termination system in DexHand Isaac environments.

## Architecture Overview

The DexHand environment uses a clean separation between **termination decisions** and **reset execution**:

- **TerminationManager**: Decides which environments should reset and why (success/failure/timeout)
- **ResetManager**: Executes physical resets (DOF states, poses, randomization)
- **BaseTask**: Coordinates between the two and manages episode progress

## Key Components

### TerminationManager
**Responsibility**: Evaluate termination conditions and decide which environments should reset

**Three Termination Types**:
- **Success**: Task completed successfully (positive reward)
- **Failure**: Task failed due to violation (negative reward)
- **Timeout**: Episode reached max length (neutral reward)

**Key Methods**:
- `evaluate(episode_step_count, success_criteria, failure_criteria)` → returns `(should_reset, termination_info, episode_rewards)`

### ResetManager
**Responsibility**: Execute physical environment resets

**Key Methods**:
- `reset_idx(env_ids)`: Reset DOF states, poses, and apply randomization
- `set_episode_step_count_buffer(buffer)`: Reference to shared episode step counter

**What it does NOT do**:
- ❌ Does not decide when to reset (no check_termination method)
- ❌ Does not increment episode progress (no increment_progress method)

### Episode Progress Management
**Handled directly in BaseTask**:
```python
# In BaseTask.post_physics_step():
self.episode_step_count += 1  # Direct increment, no method needed
```

## Clean Data Flow

```python
# In BaseTask.post_physics_step():

# 1. Update episode progress directly
self.episode_step_count += 1

# 2. Evaluate termination conditions
should_reset, termination_info, episode_rewards = self.termination_manager.evaluate(
    self.episode_step_count, success_criteria, failure_criteria
)

# 3. Apply termination rewards
for reward_type, reward_tensor in episode_rewards.items():
    self.rew_buf += reward_tensor

# 4. Reset environments that should reset
if torch.any(should_reset):
    env_ids_to_reset = torch.nonzero(should_reset).flatten()
    self.reset_manager.reset_idx(env_ids_to_reset)
    self.termination_manager.reset_tracking(env_ids_to_reset)
```

## Key Buffers

### should_reset Tensor
- **Type**: `torch.Tensor` of shape `(num_envs,)` with dtype `torch.bool`
- **Purpose**: Boolean flags indicating which environments should reset
- **Generated by**: TerminationManager.evaluate()
- **Used by**: BaseTask to determine which environments to reset

### episode_step_count Buffer
- **Type**: `torch.Tensor` of shape `(num_envs,)` with dtype `torch.long`
- **Purpose**: Tracks the number of steps in each environment's current episode
- **Managed by**: BaseTask (direct increment)
- **Reset by**: ResetManager.reset_idx() sets to 0 for reset environments

## Termination Types and Logging

The TerminationManager provides detailed termination information for rl_games TensorBoard logging:

```python
termination_info = {
    "success": success_termination,      # Boolean tensor
    "failure": failure_termination,      # Boolean tensor
    "timeout": timeout_termination,      # Boolean tensor
    "success_rate": success_count / num_envs,
    "failure_rate": failure_count / num_envs,
    "timeout_rate": timeout_count / num_envs,
}
```

This enables proper reward logging in TensorBoard because rl_games can track when episodes actually complete.

## Extending Termination Criteria

### Adding Task-Specific Success Criteria
```python
# In your task class:
def check_task_success_criteria(self):
    return {
        "object_grasped": self.check_grasp_success(),
        "target_reached": self.check_target_distance(),
    }
```

### Adding Task-Specific Failure Criteria
```python
# In your task class:
def check_task_failure_criteria(self):
    return {
        "hand_dropped": self.check_hand_height(),
        "object_dropped": self.check_object_height(),
    }
```

## Configuration

### Episode Length (Timeout)
Set in task configuration:
```yaml
# BaseTask.yaml
env:
  episodeLength: 300  # Steps before timeout termination
```

### Termination Rewards
Set in task configuration:
```yaml
# BaseTask.yaml
env:
  successReward: 10.0   # Reward for success termination
  failurePenalty: 5.0   # Penalty for failure termination (applied as negative)
  timeoutReward: 0.0    # Reward for timeout termination (usually neutral)
```

## Testing Episode Termination

You can test termination behavior with:
```bash
python examples/dexhand_test.py --episode-length 10
```

This should reset environments every 10 steps due to timeout termination, visible in the logs.

## Benefits of This Architecture

1. **No Duplication**: Single timeout check in TerminationManager (was previously duplicated)
2. **Clear Separation**: Decision logic (TerminationManager) vs execution logic (ResetManager)
3. **Better Logging**: Proper termination type tracking enables rl_games TensorBoard integration
4. **Extensible**: Easy to add new termination types or task-specific criteria
5. **Maintainable**: Single responsibility principle for each component

## References
- TerminationManager implementation: `dexhand_env/components/termination_manager.py`
- ResetManager implementation: `dexhand_env/components/reset_manager.py`
- Integration example: `dexhand_env/tasks/dexhand_base.py`
