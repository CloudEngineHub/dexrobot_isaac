# Observation System Guide

This guide explains the observation system design, implementation, and usage in the DexHand environment. The system is implemented in `ObservationEncoder` (`dexhand_env/components/observation_encoder.py`).

**Related Documentation:**
- [DOF and Action Control API](reference-dof-control-api.md) - For action space mappings
- [Coordinate Systems](reference-coordinate-systems.md) - For hand pose representations
- [Component Initialization](guide-component-initialization.md) - For system setup
- [Glossary](GLOSSARY.md) - For tensor definitions and shapes

## Design Philosophy

### Two-Level Architecture

The observation system implements a clear separation between data availability and data usage:

1. **Observation Dictionary (`obs_dict`)**: Contains ALL computed observations
   - Complete data for debugging and analysis
   - Always available regardless of configuration
   - Accessible via `ObservationEncoder.get_observations_dict()`

2. **Observation Buffer (`obs_buf`)**: Contains only selected observations
   - Configured via `policy_observation_keys` in YAML
   - Used for RL training
   - Memory-efficient tensor for policy input
   - Generated by `ObservationEncoder.concatenate_observations()`

### Key Design Principles

- **Flexibility**: Any observation can be enabled/disabled without code changes
- **Debuggability**: All observations remain accessible for inspection
- **Efficiency**: Only selected observations are concatenated for RL
- **Extensibility**: New observations can be added easily

## Privileged vs Policy-Observable Observations

### Core Concept

The observation system distinguishes between two types of observations based on their visibility to the RL policy:

#### Policy-Observable Observations
**Definition**: Information that the RL policy receives as input for decision-making.

- **Configuration**: Listed in `policy_observation_keys` in task YAML
- **Access**: Concatenated into `obs_buf` tensor that feeds into the policy network
- **Design Principle**: Should only include information available in real-world deployment
- **Examples**: Joint positions, velocities, contact binary indicators, previous actions

#### Privileged Observations
**Definition**: Internal state information used by the system but hidden from the policy.

- **Availability**: ALL observations are privileged by default - computed and stored in `obs_dict`
- **Usage**: Reward calculation, termination checking, debugging, logging, analysis
- **Design Principle**: Can include perfect simulation information not available on real robots
- **Examples**: Exact contact forces, ground truth object poses, perfect velocities, internal counters

### Why This Distinction Matters

1. **Sim-to-Real Transfer**: Policies trained with only realistic observations transfer better to real robots
2. **Training Efficiency**: Privileged information enables better reward shaping without leaking oracle knowledge to the policy
3. **Debugging**: All observations remain accessible for analysis even when not used by the policy
4. **Flexibility**: Easy to experiment with different observation sets without modifying code

### Practical Example: BlindGrasping Task

```python
# In BlindGrasping task configuration
env:
  policy_observation_keys:
    # Policy only sees proprioceptive information (blind grasping)
    - "active_finger_dof_pos"    # What the robot can sense
    - "active_finger_dof_vel"
    - "contact_binary"           # Simple touch sensors
    - "prev_actions"
    # Note: NO object_pose - policy must grasp blindly

# In task implementation
def compute_task_reward_terms(self):
    # Use privileged object pose for reward calculation
    obs_dict = self.parent.observation_encoder.get_observations_dict()
    object_height = obs_dict["object_pose"][:, 2]  # Privileged!

    # Reward based on privileged information the policy can't see
    return {"object_height": object_height}

def compute_task_terminations(self):
    # Use privileged information for success detection
    obs_dict = self.parent.observation_encoder.get_observations_dict()
    object_in_hand = obs_dict["object_in_grasp"]  # Privileged!

    return {"grasp_success": object_in_hand}, {}
```

This design allows the policy to learn blind grasping (realistic) while still having informative rewards and accurate success detection during training.

## Configuration System

### Policy Observation Configuration

Policy-observable observations are selected via the `policy_observation_keys` list in config:

```yaml
# dexhand_env/cfg/task/BaseTask.yaml
env:
  policy_observation_keys:
    - base_dof_pos           # 6D: base joint positions
    - base_dof_vel           # 6D: base joint velocities
    - active_finger_dof_pos  # 12D: active finger positions
    - active_finger_dof_vel  # 12D: active finger velocities
    - hand_pose              # 7D: position (3) + quaternion (4)
    - contact_binary         # 5D: binary contact per finger
    # Commented observations remain privileged (still computed, but hidden from policy)
    # - contact_forces       # Exact forces - too perfect for real robots
    # - object_pose          # Ground truth - not available in reality
```

### Automatic Dimension Calculation

The system performs a dry run during initialization to determine the observation space size:

```python
# During initialization, system concatenates all policy_observation_keys
# to determine final tensor dimension automatically
test_obs = self._concat_selected_observations(obs_dict)
self.num_observations = test_obs.shape[1]
```

## Implementation Architecture

### 1. Observation Computation

All observations are computed regardless of configuration:

```python
def compute_observations(self):
    # Compute ALL observations into dictionary
    obs_dict = {}

    # Hand state observations
    obs_dict["hand_pose"] = self._compute_hand_pose()
    obs_dict["hand_vel"] = self._compute_hand_velocity()

    # DOF observations
    obs_dict["base_dof_pos"] = self._compute_base_dof_pos()
    obs_dict["finger_dof_pos"] = self._compute_finger_dof_pos()

    # Contact observations
    obs_dict["contact_forces"] = self._compute_contact_forces()

    # Task-specific observations
    task_obs = self._compute_task_observations()
    obs_dict.update(task_obs)

    return obs_dict
```

### 2. Selective Concatenation for Policy

Only policy-observable observations are concatenated:

```python
def _concatenate_observations(self, obs_dict):
    obs_list = []

    # Only concatenate policy-observable keys
    for key in self.policy_observation_keys:
        if key in obs_dict:
            obs_list.append(obs_dict[key])

    # Create final observation buffer for policy
    self.obs_buf = torch.cat(obs_list, dim=-1)
```

### 3. Shape Management

Each observation component has a defined shape:

```python
OBSERVATION_SHAPES = {
    "hand_pose": 7,                # pos(3) + quat(4) - raw pose
    "hand_pose_arr_aligned": 7,    # pos(3) + quat(4) - aligned to ARR DOFs
    "hand_vel": 6,                 # lin(3) + ang(3)
    "base_dof_pos": 6,             # 6 base DOFs
    "base_dof_vel": 6,
    "finger_dof_pos": 20,          # 20 finger DOFs
    "finger_dof_vel": 20,
    "fingertip_poses": 35,         # 5 tips × 7D
    "contact_forces": 15,          # 5 tips × 3D
    # ... more observations
}
```

## Accessor System

### Dictionary Access for Privileged Information

Access any observation (privileged or policy-observable):

```python
# Get full observation dictionary with ALL observations
obs_dict = env.get_observations_dict()

# Access privileged observations for analysis
object_pose = obs_dict["object_pose"]        # Privileged - not in policy
contact_forces = obs_dict["contact_forces"]  # Privileged - exact forces

# Access policy-observable observations
hand_pose = obs_dict["hand_pose"]           # Also in policy input
contact_binary = obs_dict["contact_binary"] # Also in policy input

# Useful for debugging and reward computation
print(f"Object height (privileged): {object_pose[:, 2]}")
print(f"Exact forces (privileged): {contact_forces}")
```

### Buffer Access for Policy Input

Get only policy-observable observations:

```python
# Get observation buffer for policy
obs = env.obs_buf  # Shape: (num_envs, obs_dim)

# obs contains ONLY policy-observable observations
# in the order specified by policy_observation_keys
# No privileged information leaks to the policy here
```

### Direct Component Access

Access specific observation components:

```python
# Via observation encoder
encoder = env.observation_encoder

# Get specific observations
hand_pose = encoder.get_hand_pose()
fingertip_poses = encoder.get_fingertip_poses()
dof_positions = encoder.get_dof_positions()
```

## Adding New Observations

### 1. Define Computation Method

```python
def _compute_my_observation(self):
    # Compute your observation
    my_obs = some_computation()

    # Ensure correct shape: (num_envs, obs_dim)
    return my_obs.view(self.num_envs, -1)
```

### 2. Add to Dictionary

```python
def _compute_default_observations(self):
    obs_dict = {}

    # Existing observations...

    # Add your observation
    obs_dict["my_observation"] = self._compute_my_observation()

    return obs_dict
```

### 3. Define Shape

```python
# In observation_encoder.py
OBSERVATION_SHAPES = {
    # Existing shapes...
    "my_observation": 10,  # Your observation dimension
}
```

### 4. Enable in Config

```yaml
env:
  observation_keys:
    - hand_pose
    - my_observation  # Add to enable for RL
```

## Common Observation Types

### State Observations
- `hand_pose`: Hand base position and orientation (raw from rigid body state)
- `hand_pose_arr_aligned`: Hand pose with orientation aligned to ARR DOFs (compensates for built-in 90° Y rotation)
- `hand_vel`: Hand base linear and angular velocity
- `base_dof_pos/vel`: Base joint positions and velocities
- `finger_dof_pos/vel`: Finger joint positions and velocities

### ARR-Aligned Pose Details

The `hand_pose_arr_aligned` observation addresses a coordinate system issue in the floating hand model:

**Problem**: Due to the floating hand model design, the hand is mounted with a built-in 90° Y-axis rotation. When ARRx=ARRy=ARRz=0, the raw hand quaternion is approximately [0, 0.707, 0, 0.707] instead of identity [0, 0, 0, 1].

**Solution**: The ARR-aligned pose compensates for this rotation:
- Position: Same as raw hand pose
- Orientation: Multiplied by inverse of the built-in rotation
- Result: Quaternion values that directly correspond to ARRx, ARRy, ARRz DOF values

**Usage Example**:
```python
obs_dict = env.get_observations_dict()

# Raw pose - includes built-in 90° Y rotation
raw_quat = obs_dict["hand_pose"][:, 3:7]  # [0, 0.707, 0, 0.707] when ARR=0

# ARR-aligned pose - compensated orientation
aligned_quat = obs_dict["hand_pose_arr_aligned"][:, 3:7]  # [0, 0, 0, 1] when ARR=0

# Convert to Euler angles for intuitive interpretation
from scipy.spatial.transform import Rotation
euler = Rotation.from_quat(aligned_quat).as_euler('xyz', degrees=True)
# euler ≈ [0°, 0°, 0°] when ARRx=ARRy=ARRz=0
```

### Contact Observations
- `contact_forces`: 3D force vectors at fingertips
- `contact_binary`: Binary contact indicators
- `contact_locations`: Contact point positions

### Relative Observations
- `fingertip_poses`: Fingertip poses in world frame
- `fingertip_poses_hand_frame`: In hand frame
- `object_pose_hand_frame`: Object relative to hand

### Control Observations
- `dof_targets`: Current DOF position targets
- `last_actions`: Previous step's actions
- `action_deltas`: Change in actions

## Performance Considerations

### Memory Efficiency
- Only enabled observations are stored in `obs_buf`
- Dictionary overhead is minimal (references only)
- Batch operations for all environments

### Computation Efficiency
- Vectorized operations for all observations
- No loops over environments
- Efficient tensor reshaping

### Repository-Specific Configuration

**Observation Selection**: Configured via `policy_observation_keys` in task YAML:
```yaml
# In task/BaseTask.yaml or task-specific configs
env:
  policy_observation_keys:
    - "base_dof_pos"            # Base DOF positions
    - "active_finger_dof_pos"   # Active finger positions
    - "hand_pose"               # Hand root pose
    - "contact_binary"          # Contact indicators
    # Add/remove keys to customize observation tensor
```

**Observation Size Determination**:
The system performs a dry run during initialization to concatenate all configured observations and determine the final tensor size automatically. No manual shape registration needed.

## Troubleshooting Repository-Specific Issues

### Zero Values in Observations
```python
# Check tensor refresh order in PhysicsManager
self.gym.refresh_dof_state_tensor(self.sim)  # Must be called before reading
positions = self.dof_pos  # Now contains valid data
```

### Adding Task-Specific Observations
```python
# In task's compute_task_observations()
def compute_task_observations(self, obs_dict):
    obs_dict["target_distance"] = torch.norm(
        self.target_pos - self.hand_pos, dim=-1
    )
    return obs_dict

## Example Usage

### Configuring Policy vs Privileged Observations
```yaml
# task/MyGraspingTask.yaml
env:
  policy_observation_keys:
    # Realistic sensor data for policy
    - "active_finger_dof_pos"
    - "active_finger_dof_vel"
    - "contact_binary"
    - "prev_actions"
    # NOT including: object_pose, exact_forces, perfect_velocities
```

### Using Privileged Information in Rewards
```python
class MyGraspingTask(BaseTask):
    def compute_task_reward_terms(self):
        # Get ALL observations including privileged
        obs_dict = self.parent.observation_encoder.get_observations_dict()

        # Use privileged info for reward shaping
        object_height = obs_dict["object_pose"][:, 2]  # Privileged!
        exact_forces = obs_dict["contact_forces"]      # Privileged!

        # Policy never sees these, but they guide learning
        rewards = {
            "lift_reward": object_height * 10.0,
            "gentle_grasp": -torch.norm(exact_forces, dim=-1)
        }
        return rewards
```

### Debugging with Privileged Observations
```python
# During development, check both policy and privileged observations
obs_dict = env.get_observations_dict()

# What the policy sees
policy_obs = env.obs_buf
print(f"Policy input shape: {policy_obs.shape}")
print(f"Policy sees these keys: {env.policy_observation_keys}")

# What's actually happening (privileged)
print(f"True object position: {obs_dict['object_pose']}")
print(f"Exact contact forces: {obs_dict['contact_forces']}")
print(f"Perfect velocities: {obs_dict['object_vel']}")

# Verify policy can't access privileged info
assert "object_pose" not in env.policy_observation_keys
```

### Custom Task with Mixed Observations
```python
def compute_task_observations(self, obs_dict):
    # Add task-specific observations (all privileged by default)
    obs_dict["object_distance"] = torch.norm(
        obs_dict["object_pose"][:, :3] - obs_dict["hand_pose"][:, :3],
        dim=-1
    )
    obs_dict["grasp_aperture"] = self._compute_grasp_aperture()
    obs_dict["object_velocity"] = self._get_object_velocity()

    # These are only policy-observable if listed in policy_observation_keys
    return obs_dict
```

## Related Documentation

- **Component Initialization**: [`guide-component-initialization.md`](guide-component-initialization.md) - Dependency management and initialization order
- **Design Decisions**: [`DESIGN_DECISIONS.md`](DESIGN_DECISIONS.md) - Critical design caveats
- **DOF/Action Reference**: [`reference-dof-control-api.md`](reference-dof-control-api.md) - DOF indices and action mapping
- **Physics Implementation**: [`reference-physics-implementation.md`](reference-physics-implementation.md) - Technical implementation details
