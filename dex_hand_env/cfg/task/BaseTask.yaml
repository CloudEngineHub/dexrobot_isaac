# Base Task Configuration File

env:
  # Task parameters
  episodeLength: 300              # Maximum number of steps per episode

  # Action space configuration
  controlMode: "position_delta"   # position or position_delta
  policyControlsHandBase: true    # Whether the policy controls the hand base (6 DOFs) - when false, use rule-based control
  policyControlsFingers: true     # Whether the policy controls the finger joints (12 controls) - when false, use rule-based control

  # Default targets for uncontrolled DOFs (fallback if task doesn't provide them)
  # These are used only if the task's get_task_dof_targets method doesn't return targets
  # NOTE: Base targets are RELATIVE displacements from spawn position (0 = stay at spawn point)
  defaultBaseTargets: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Default base targets [x, y, z, rx, ry, rz]
  defaultFingerTargets: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  # Default finger targets (12 controls)

  # Component-wise velocity limits for position_delta mode action scaling
  # These limits are applied PER COMPONENT (each joint/axis independently)
  maxFingerJointVelocity: 1.0     # Maximum velocity for each finger joint (rad/s) - applied per joint
  maxBaseLinearVelocity: 0.5      # Maximum velocity for each base linear axis (m/s) - applied per x/y/z axis
  maxBaseAngularVelocity: 1.5     # Maximum velocity for each base angular axis (rad/s) - applied per rx/ry/rz axis

  # Viewer synchronization
  enableViewerSync: true          # Synchronize viewer to real-time animation speed

  # Controller parameters now loaded from MJCF model

  # Initial pose
  initialHandPos: [0.0, 0.0, 0.5] # Initial hand position
  initialHandRot: [0.0, 0.0, 0.0, 1.0]  # Initial hand rotation (quaternion)

  # Observation configuration
  observationKeys:              # List of observation components to include in final observation tensor
    - "base_dof_pos"            # Base DOF positions (6 DOFs: x, y, z, rx, ry, rz)
    - "base_dof_vel"            # Base DOF velocities (6 DOFs: x, y, z, rx, ry, rz)
    - "active_finger_dof_pos"   # Active finger DOF positions (12 active finger controls) - for RL
    - "active_finger_dof_vel"   # Active finger DOF velocities (12 active finger controls) - for RL
    - "hand_pose"               # Hand root pose (position + orientation)
    - "contact_forces"          # Contact forces (3D force for each finger)
    - "prev_actions"            # Previous actions
    - "base_dof_target"         # Base DOF targets (6 DOFs: x, y, z, rx, ry, rz)
    - "active_finger_dof_target" # Active finger DOF targets (12 active finger controls) - for RL
    - "contact_force_magnitude" # Contact force magnitude (scalar for each finger)
    - "fingertip_poses_world"   # Fingertip poses in world frame (5 fingers × 7 pose dimensions = 35)
    - "fingertip_poses_hand"    # Fingertip poses in hand frame (5 fingers × 7 pose dimensions = 35)
    - "fingerpad_poses_world"   # Fingerpad poses in world frame (5 fingers × 7 pose dimensions = 35)
    - "fingerpad_poses_hand"    # Fingerpad poses in hand frame (5 fingers × 7 pose dimensions = 35)

    # Disabled observation keys (available in obs_dict but not included in RL observation tensor)
    # Uncomment any of these to include them in the RL observation tensor:

    # Note: The following are always available in obs_dict for semantic access regardless of whether
    # they are included in observationKeys. They are commented out to avoid redundancy in RL tensor.

    # - "all_finger_dof_pos"     # All finger DOF positions (20 finger DOFs) - semantic access only
    # - "all_finger_dof_vel"     # All finger DOF velocities (20 finger DOFs) - semantic access only
    # - "all_finger_dof_target"  # All finger DOF targets (20 finger DOFs) - semantic access only

  # Reward parameters
  rewardScales:
    pose: 1.0                     # Scale for pose matching reward
    action: 0.1                   # Scale for action smoothness reward
    energy: 0.1                   # Scale for energy efficiency reward
    contact: 0.5                  # Scale for contact reward
    success: 10.0                 # Scale for task success reward

sim:
  dt: 0.0083                      # Reduced timestep for rendering mode stability (120 Hz)
  substeps: 4                     # Increased substeps for better convergence with rendering
  physx:
    solver_type: 1                # 0: PCG, 1: TGS
    num_position_iterations: 32   # Significantly increased for rendering mode stability
    num_velocity_iterations: 0    # Set to 0 based on NVIDIA recommendations (doc: negatively impacts convergence)
    contact_offset: 0.005         # Balanced value between stability and performance
    rest_offset: 0.002            # Increased rest offset for better contact stability
    contact_collection: 1         # 1: CC_LAST_SUBSTEP (critical for GPU pipeline)
    default_buffer_size_multiplier: 8.0  # Increased rest offset for better contact stability
    gpu_contact_pairs_per_env: 1024  # Contact pairs per environment (will be multiplied by num_envs)
    always_use_articulations: true  # Always use articulations for stability
    bounce_threshold_velocity: 0.1  # Reduced for more stable contact behavior
    max_depenetration_velocity: 5.0  # Further reduced to minimize aggressive corrections
    num_threads: 4                # Standard CPU threads

# These parameters are passed to training algorithms
task:
  randomize: false                # Whether to use domain randomization
